{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below we are building a __Spanish to English__ machine translation model which will be  used in 2_Classification_Model_Mac.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import unidecode\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences # for padding\n",
    "from keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint # to prevent overfitting and save the best model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Spanish to English Sentences\n",
    "\n",
    "The dataset can be downloaded from here : http://www.manythings.org/anki/spa-eng.zip\n",
    "\n",
    "Other similar datasets for different languages can be found at http://www.manythings.org/anki/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read the text file\n",
    "def read_text(filename):\n",
    "        # open the file\n",
    "        file = open(filename, mode='rt', encoding='utf-8')\n",
    "        # read all text\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a text into sentences\n",
    "def to_lines(text):\n",
    "      sents = text.strip().split('\\n')\n",
    "      sents = [i.split('\\t') for i in sents]\n",
    "      return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "txt = read_text(\"data/spa.txt\")\n",
    "\n",
    "# convert text into list of \n",
    "sp_eng = to_lines(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123379"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check count of sentence pairs in the data\n",
    "len(sp_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into array\n",
    "sp_eng = np.array(sp_eng)\n",
    "\n",
    "# empty lists\n",
    "eng_l = []\n",
    "sp_l = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in sp_eng[:,0]:\n",
    "      eng_l.append(i)\n",
    "\n",
    "for i in sp_eng[:,1]:\n",
    "      sp_l.append(i)\n",
    "        \n",
    "data = pd.DataFrame({'spa':sp_l, 'eng':eng_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spa</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123374</th>\n",
       "      <td>Hay madres y padres que se quedan despiertos después de que sus hijos se hayan dormido y se preguntan cómo conseguir pagar la hipoteca o las facturas del médico, o cómo ahorrar el suficiente diner...</td>\n",
       "      <td>There are mothers and fathers who will lie awake after the children fall asleep and wonder how they'll make the mortgage, or pay their doctor's bills, or save enough for their child's college educ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123375</th>\n",
       "      <td>Una huella de carbono es la cantidad de contaminación de dióxido de carbono que producimos como producto de nuestras actividades. Algunas personas intentan reducir su huella de carbono porque está...</td>\n",
       "      <td>A carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities. Some people try to reduce their carbon footprint because they are concerned about climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123376</th>\n",
       "      <td>Como suele haber varias páginas web sobre cualquier tema, normalmente sólo le doy al botón de retroceso cuando entro en una página web que tiene anuncios en ventanas emergentes. Simplemente voy a ...</td>\n",
       "      <td>Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123377</th>\n",
       "      <td>Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra ...</td>\n",
       "      <td>If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123378</th>\n",
       "      <td>Puede que sea imposible obtener un corpus completamente libre de errores debido a la naturaleza de este tipo de esfuerzo de colaboración. Sin embargo, si animamos a los miembros a contribuir frase...</td>\n",
       "      <td>It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                            spa  \\\n",
       "123374  Hay madres y padres que se quedan despiertos después de que sus hijos se hayan dormido y se preguntan cómo conseguir pagar la hipoteca o las facturas del médico, o cómo ahorrar el suficiente diner...   \n",
       "123375  Una huella de carbono es la cantidad de contaminación de dióxido de carbono que producimos como producto de nuestras actividades. Algunas personas intentan reducir su huella de carbono porque está...   \n",
       "123376  Como suele haber varias páginas web sobre cualquier tema, normalmente sólo le doy al botón de retroceso cuando entro en una página web que tiene anuncios en ventanas emergentes. Simplemente voy a ...   \n",
       "123377  Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra ...   \n",
       "123378  Puede que sea imposible obtener un corpus completamente libre de errores debido a la naturaleza de este tipo de esfuerzo de colaboración. Sin embargo, si animamos a los miembros a contribuir frase...   \n",
       "\n",
       "                                                                                                                                                                                                            eng  \n",
       "123374  There are mothers and fathers who will lie awake after the children fall asleep and wonder how they'll make the mortgage, or pay their doctor's bills, or save enough for their child's college educ...  \n",
       "123375  A carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities. Some people try to reduce their carbon footprint because they are concerned about climat...  \n",
       "123376  Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Goo...  \n",
       "123377  If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until t...  \n",
       "123378  It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages r...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates from the Spanish sentences\n",
    "data.drop_duplicates(subset=['spa'],inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Let's Preprocess the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the text\n",
    "def cleaner(text):\n",
    "    newString = text.lower()\n",
    "    unaccented_string = unidecode.unidecode(newString)\n",
    "    newString = re.sub(\"'\",'', unaccented_string) \n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = newString.split()\n",
    "    return (\" \".join(tokens)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess english text\n",
    "cleaned_eng = []\n",
    "for t in data['eng']:\n",
    "    cleaned_eng.append(cleaner(t)) \n",
    "    \n",
    "# preprocess Spanish text\n",
    "cleaned_spa = []\n",
    "for t in data['spa']:\n",
    "    cleaned_spa.append(cleaner(t)) \n",
    "\n",
    "    \n",
    "data['cleaned_eng']=cleaned_eng\n",
    "data['cleaned_spa']=cleaned_spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spa</th>\n",
       "      <th>eng</th>\n",
       "      <th>cleaned_eng</th>\n",
       "      <th>cleaned_spa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123374</th>\n",
       "      <td>Hay madres y padres que se quedan despiertos después de que sus hijos se hayan dormido y se preguntan cómo conseguir pagar la hipoteca o las facturas del médico, o cómo ahorrar el suficiente diner...</td>\n",
       "      <td>There are mothers and fathers who will lie awake after the children fall asleep and wonder how they'll make the mortgage, or pay their doctor's bills, or save enough for their child's college educ...</td>\n",
       "      <td>there are mothers and fathers who will lie awake after the children fall asleep and wonder how theyll make the mortgage or pay their doctors bills or save enough for their childs college education</td>\n",
       "      <td>hay madres y padres que se quedan despiertos despues de que sus hijos se hayan dormido y se preguntan como conseguir pagar la hipoteca o las facturas del medico o como ahorrar el suficiente dinero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123375</th>\n",
       "      <td>Una huella de carbono es la cantidad de contaminación de dióxido de carbono que producimos como producto de nuestras actividades. Algunas personas intentan reducir su huella de carbono porque está...</td>\n",
       "      <td>A carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities. Some people try to reduce their carbon footprint because they are concerned about climat...</td>\n",
       "      <td>a carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities some people try to reduce their carbon footprint because they are concerned about climate...</td>\n",
       "      <td>una huella de carbono es la cantidad de contaminacion de dioxido de carbono que producimos como producto de nuestras actividades algunas personas intentan reducir su huella de carbono porque estan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123376</th>\n",
       "      <td>Como suele haber varias páginas web sobre cualquier tema, normalmente sólo le doy al botón de retroceso cuando entro en una página web que tiene anuncios en ventanas emergentes. Simplemente voy a ...</td>\n",
       "      <td>Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Goo...</td>\n",
       "      <td>since there are usually multiple websites on any given topic i usually just click the back button when i arrive on any webpage that has pop up advertising i just go to the next page found by googl...</td>\n",
       "      <td>como suele haber varias paginas web sobre cualquier tema normalmente solo le doy al boton de retroceso cuando entro en una pagina web que tiene anuncios en ventanas emergentes simplemente voy a la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123377</th>\n",
       "      <td>Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra ...</td>\n",
       "      <td>If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until t...</td>\n",
       "      <td>if you want to sound like a native speaker you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until th...</td>\n",
       "      <td>si quieres sonar como un hablante nativo debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123378</th>\n",
       "      <td>Puede que sea imposible obtener un corpus completamente libre de errores debido a la naturaleza de este tipo de esfuerzo de colaboración. Sin embargo, si animamos a los miembros a contribuir frase...</td>\n",
       "      <td>It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages r...</td>\n",
       "      <td>it may be impossible to get a completely error free corpus due to the nature of this kind of collaborative effort however if we encourage members to contribute sentences in their own languages rat...</td>\n",
       "      <td>puede que sea imposible obtener un corpus completamente libre de errores debido a la naturaleza de este tipo de esfuerzo de colaboracion sin embargo si animamos a los miembros a contribuir frases ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                            spa  \\\n",
       "123374  Hay madres y padres que se quedan despiertos después de que sus hijos se hayan dormido y se preguntan cómo conseguir pagar la hipoteca o las facturas del médico, o cómo ahorrar el suficiente diner...   \n",
       "123375  Una huella de carbono es la cantidad de contaminación de dióxido de carbono que producimos como producto de nuestras actividades. Algunas personas intentan reducir su huella de carbono porque está...   \n",
       "123376  Como suele haber varias páginas web sobre cualquier tema, normalmente sólo le doy al botón de retroceso cuando entro en una página web que tiene anuncios en ventanas emergentes. Simplemente voy a ...   \n",
       "123377  Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra ...   \n",
       "123378  Puede que sea imposible obtener un corpus completamente libre de errores debido a la naturaleza de este tipo de esfuerzo de colaboración. Sin embargo, si animamos a los miembros a contribuir frase...   \n",
       "\n",
       "                                                                                                                                                                                                            eng  \\\n",
       "123374  There are mothers and fathers who will lie awake after the children fall asleep and wonder how they'll make the mortgage, or pay their doctor's bills, or save enough for their child's college educ...   \n",
       "123375  A carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities. Some people try to reduce their carbon footprint because they are concerned about climat...   \n",
       "123376  Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Goo...   \n",
       "123377  If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until t...   \n",
       "123378  It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages r...   \n",
       "\n",
       "                                                                                                                                                                                                    cleaned_eng  \\\n",
       "123374     there are mothers and fathers who will lie awake after the children fall asleep and wonder how theyll make the mortgage or pay their doctors bills or save enough for their childs college education   \n",
       "123375  a carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities some people try to reduce their carbon footprint because they are concerned about climate...   \n",
       "123376  since there are usually multiple websites on any given topic i usually just click the back button when i arrive on any webpage that has pop up advertising i just go to the next page found by googl...   \n",
       "123377  if you want to sound like a native speaker you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until th...   \n",
       "123378  it may be impossible to get a completely error free corpus due to the nature of this kind of collaborative effort however if we encourage members to contribute sentences in their own languages rat...   \n",
       "\n",
       "                                                                                                                                                                                                    cleaned_spa  \n",
       "123374  hay madres y padres que se quedan despiertos despues de que sus hijos se hayan dormido y se preguntan como conseguir pagar la hipoteca o las facturas del medico o como ahorrar el suficiente dinero...  \n",
       "123375  una huella de carbono es la cantidad de contaminacion de dioxido de carbono que producimos como producto de nuestras actividades algunas personas intentan reducir su huella de carbono porque estan...  \n",
       "123376  como suele haber varias paginas web sobre cualquier tema normalmente solo le doy al boton de retroceso cuando entro en una pagina web que tiene anuncios en ventanas emergentes simplemente voy a la...  \n",
       "123377  si quieres sonar como un hablante nativo debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra v...  \n",
       "123378  puede que sea imposible obtener un corpus completamente libre de errores debido a la naturaleza de este tipo de esfuerzo de colaboracion sin embargo si animamos a los miembros a contribuir frases ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(data['cleaned_spa'],data['cleaned_eng'], test_size = 0.2, random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving validation data for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing validation data into new variables for later use\n",
    "x_val_original = x_val\n",
    "y_val_original = y_val\n",
    "\n",
    "# reset index\n",
    "y_val_original.reset_index(inplace=True, drop=True)\n",
    "x_val_original.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of word-frequency pairs for the spanish text data in the training set (*x_tr*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "# create word-frequency pair dictionary\n",
    "source_word_freq = build_vocab(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find proportion of tokens occurring less than a threshold value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 42.60680034873584\n",
      "Total Coverage of rare words: 1.6975705323709018\n"
     ]
    }
   ],
   "source": [
    "# set threshold value for rare words\n",
    "thresh=2\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in source_word_freq.items():\n",
    "  tot_cnt=tot_cnt+1\n",
    "  tot_freq=tot_freq+value\n",
    "  if(value<thresh):\n",
    "    cnt=cnt+1\n",
    "    freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Build a word-index pair dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign index, starting from 2\n",
    "source_word_index={}\n",
    "cnt=2\n",
    "for key,value in source_word_freq.items():\n",
    "  # add token if it is not rare  \n",
    "  if(value>=thresh):\n",
    "    source_word_index[key]=cnt  \n",
    "    cnt=cnt+1\n",
    "\n",
    "# assign index to \"padding\" and \"unknown\" tokens\n",
    "source_word_index['<pad>']=0\n",
    "source_word_index['<unk>']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Create integer-sequences from the spanish sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab=[key for key,value in source_word_index.items()]  #spanish vocabulary\n",
    "\n",
    "# sentences to integer sequences (Spanish sentences - training data)\n",
    "source_seq_tr=[]\n",
    "for i in x_tr:\n",
    "  seq=[]\n",
    "  for j in i.split():\n",
    "    if(j not in source_vocab):\n",
    "      seq.append(source_word_index['<unk>'])\n",
    "    elif(source_word_freq[j]<thresh):\n",
    "      seq.append(source_word_index['<unk>'])\n",
    "    else:\n",
    "      seq.append(source_word_index[j])\n",
    "  source_seq_tr.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
       " [12, 13, 14, 15, 16, 17, 5, 6, 18],\n",
       " [3, 19, 20, 13, 21, 22],\n",
       " [23, 1, 24, 25, 26, 14, 27, 1],\n",
       " [28, 29, 30, 31, 8, 23, 32],\n",
       " [33, 34, 35, 36, 5, 37, 38],\n",
       " [39, 40, 41, 28],\n",
       " [42, 26, 13, 43, 44, 45, 46],\n",
       " [2, 24, 47, 48, 49, 20, 50, 51, 1, 52, 53],\n",
       " [54, 55, 56, 57]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_seq_tr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences to integer sequences (Spanish sentences - validation data)\n",
    "source_seq_val=[]\n",
    "for i in x_val:\n",
    "  seq=[]\n",
    "  for j in i.split():\n",
    "    if(j not in source_vocab):\n",
    "      seq.append(source_word_index['<unk>'])\n",
    "    elif(source_word_freq[j]<thresh):\n",
    "      seq.append(source_word_index['<unk>'])\n",
    "    else:\n",
    "      seq.append(source_word_index[j])\n",
    "  source_seq_val.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english word-frequency dictionary\n",
    "target_word_freq = build_vocab(y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of rare words: 33.67548037444572\n",
      "Total Coverage of rare words: 0.6888975120065715\n"
     ]
    }
   ],
   "source": [
    "thresh=2 # set threshold count \n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in target_word_freq.items():\n",
    "  tot_cnt=tot_cnt+1\n",
    "  tot_freq=tot_freq+value\n",
    "  if(value<thresh):\n",
    "    cnt=cnt+1\n",
    "    freq=freq+value\n",
    "    \n",
    "print(\"Vocabulary of rare words:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Build a dictionary that assigns index to every word in the vocabulary by removing the rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_index={}\n",
    "\n",
    "cnt=4 # start assigning index from 4 \n",
    "for key,value in target_word_freq.items():\n",
    "  if(value>=thresh):\n",
    "    target_word_index[key]=cnt  \n",
    "    cnt=cnt+1\n",
    "\n",
    "# Assign index to \"padding\" and \"unknown\" token\n",
    "target_word_index['<pad>']=0\n",
    "target_word_index['<unk>']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Create integer-sequences from the English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab=[key for key,value in target_word_index.items()]  #English vocabulary\n",
    "\n",
    "# sentences to integer sequences (Engish sentences - training data)\n",
    "target_seq_tr=[]\n",
    "for i in y_tr:\n",
    "  seq=[]\n",
    "  for j in i.split():\n",
    "    if(j not in target_vocab):\n",
    "      seq.append(target_word_index['<unk>'])\n",
    "    elif(target_word_freq[j]<thresh):\n",
    "      seq.append(target_word_index['<unk>'])\n",
    "    else:\n",
    "      seq.append(target_word_index[j])\n",
    "  target_seq_tr.append(seq)\n",
    "\n",
    "# sentences to integer sequences (Engish sentences - validation data)\n",
    "target_seq_val=[]\n",
    "for i in y_val:\n",
    "  seq=[]\n",
    "  for j in i.split():\n",
    "    if(j not in target_vocab):\n",
    "      seq.append(target_word_index['<unk>'])\n",
    "    elif(target_word_freq[j]<thresh):\n",
    "      seq.append(target_word_index['<unk>'])\n",
    "    else:\n",
    "      seq.append(target_word_index[j])\n",
    "  target_seq_val.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Pad the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check sentence length\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eng_word_count = []\n",
    "spa_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['eng']:\n",
    "      eng_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['spa']:\n",
    "      spa_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'eng':eng_word_count, 'spa':spa_word_count})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set maximum length for English and Spanish sentences\n",
    "max_eng_len=15\n",
    "max_spa_len=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad Spanish sequences\n",
    "x_tr = pad_sequences(source_seq_tr,  maxlen=max_spa_len, padding='post', truncating='post')\n",
    "x_val = pad_sequences(source_seq_val, maxlen=max_spa_len, padding='post', truncating='post')\n",
    "x_voc = len(source_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad English sequences\n",
    "y_tr_pad    =   pad_sequences(target_seq_tr, maxlen=max_eng_len, padding='post', truncating=\"post\")\n",
    "y_val_pad   =   pad_sequences(target_seq_val, maxlen=max_eng_len, padding='post', truncating=\"post\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens for the training data\n",
    "target_word_index['<start>']=2\n",
    "target_word_index['<end>']=3\n",
    "\n",
    "# Add start token to target sequences\n",
    "y_tr_start=[]\n",
    "for i in y_tr_pad:\n",
    "  temp=np.insert(i,0,target_word_index['<start>'])\n",
    "  y_tr_start.append(temp)\n",
    "\n",
    "#Add end token to target sequences \n",
    "y_tr=[]\n",
    "for i in y_tr_start:\n",
    "  if(0 in list(i)):\n",
    "    temp=np.insert(i,list(i).index(0),target_word_index['<end>'])\n",
    "  else:\n",
    "    temp=np.insert(i,len(i),target_word_index['<end>'])\n",
    "  y_tr.append(temp)\n",
    "  \n",
    "y_tr=np.array(y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_voc = len(target_word_index)   #size of english vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens for the validation data\n",
    "\n",
    "# Add start token to target sequences\n",
    "y_val_start=[]\n",
    "for i in y_val_pad:\n",
    "  temp=np.insert(i,0,target_word_index['<start>'])\n",
    "  y_val_start.append(temp)\n",
    "\n",
    "# Add end token to target sequences \n",
    "y_val=[]\n",
    "for i in y_val_start:\n",
    "  if(0 in list(i)):\n",
    "    temp=np.insert(i,list(i).index(0),target_word_index['<end>'])\n",
    "  else:\n",
    "    temp=np.insert(i,len(i),target_word_index['<end>'])\n",
    "  y_val.append(temp)\n",
    "  \n",
    "y_val=np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update maximum length\n",
    "max_eng_len=max_eng_len+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of hidden units\n",
    "latent_dim = 150 \n",
    "\n",
    "# specify length of word embeddings \n",
    "embedding_dim=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Encoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0919 06:58:42.146799 23076 deprecation_wrapper.py:119] From C:\\Users\\seqpr\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0919 06:58:42.167680 23076 deprecation_wrapper.py:119] From C:\\Users\\seqpr\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0919 06:58:42.173289 23076 deprecation_wrapper.py:119] From C:\\Users\\seqpr\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(max_spa_len,))\n",
    "enc_emb =  Embedding(x_voc, embedding_dim, trainable=True)(encoder_inputs)\n",
    "\n",
    "encoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(__encoder_outputs__: all the hidden states, __state_h__: final hidden state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Architecture, using \"encoder_states\" as initial state.\n",
    "decoder_inputs = Input(shape=(max_eng_len-1,))\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_output, decoder_state_h, decoder_state_c = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_output) # probability distribution of vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 15, 200)      2633600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 16, 200)      1616200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 15, 150), (N 210600      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 16, 150), (N 210600      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 16, 8081)     1220231     lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,891,231\n",
      "Trainable params: 5,891,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0919 06:58:43.032659 23076 deprecation_wrapper.py:119] From C:\\Users\\seqpr\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0919 06:58:43.056264 23076 deprecation_wrapper.py:119] From C:\\Users\\seqpr\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3, min_delta=0.0001)\n",
    "mc = ModelCheckpoint('pickle_and_trained_models/best_model_v4.h5',monitor='val_loss',mode='min', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0919 06:58:43.316297 23076 deprecation.py:323] From C:\\Users\\seqpr\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0919 06:58:45.278910 23076 deprecation_wrapper.py:119] From C:\\Users\\seqpr\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 93935 samples, validate on 23484 samples\n",
      "Epoch 1/100\n",
      "93935/93935 [==============================] - 477s 5ms/step - loss: 2.9385 - val_loss: 2.3875\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.38752, saving model to best_model_v4.h5\n",
      "Epoch 2/100\n",
      "93935/93935 [==============================] - 492s 5ms/step - loss: 2.2687 - val_loss: 2.1420\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.38752 to 2.14202, saving model to best_model_v4.h5\n",
      "Epoch 3/100\n",
      "93935/93935 [==============================] - 482s 5ms/step - loss: 2.0161 - val_loss: 1.9214\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.14202 to 1.92141, saving model to best_model_v4.h5\n",
      "Epoch 4/100\n",
      "93935/93935 [==============================] - 401s 4ms/step - loss: 1.8530 - val_loss: 1.7978\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.92141 to 1.79782, saving model to best_model_v4.h5\n",
      "Epoch 5/100\n",
      "93935/93935 [==============================] - 297s 3ms/step - loss: 1.7260 - val_loss: 1.7024\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.79782 to 1.70235, saving model to best_model_v4.h5\n",
      "Epoch 6/100\n",
      "93935/93935 [==============================] - 301s 3ms/step - loss: 1.6145 - val_loss: 1.5966\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.70235 to 1.59663, saving model to best_model_v4.h5\n",
      "Epoch 7/100\n",
      "93935/93935 [==============================] - 302s 3ms/step - loss: 1.5211 - val_loss: 1.5185\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.59663 to 1.51853, saving model to best_model_v4.h5\n",
      "Epoch 8/100\n",
      "93935/93935 [==============================] - 311s 3ms/step - loss: 1.4372 - val_loss: 1.4509\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.51853 to 1.45093, saving model to best_model_v4.h5\n",
      "Epoch 9/100\n",
      "93935/93935 [==============================] - 304s 3ms/step - loss: 1.3563 - val_loss: 1.3835\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.45093 to 1.38350, saving model to best_model_v4.h5\n",
      "Epoch 10/100\n",
      "93935/93935 [==============================] - 305s 3ms/step - loss: 1.2806 - val_loss: 1.3247\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.38350 to 1.32470, saving model to best_model_v4.h5\n",
      "Epoch 11/100\n",
      "93935/93935 [==============================] - 316s 3ms/step - loss: 1.2101 - val_loss: 1.2702\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.32470 to 1.27016, saving model to best_model_v4.h5\n",
      "Epoch 12/100\n",
      "93935/93935 [==============================] - 347s 4ms/step - loss: 1.1454 - val_loss: 1.2232\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.27016 to 1.22321, saving model to best_model_v4.h5\n",
      "Epoch 13/100\n",
      "93935/93935 [==============================] - 337s 4ms/step - loss: 1.0852 - val_loss: 1.1854\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.22321 to 1.18540, saving model to best_model_v4.h5\n",
      "Epoch 14/100\n",
      "93935/93935 [==============================] - 365s 4ms/step - loss: 1.0303 - val_loss: 1.1445\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.18540 to 1.14446, saving model to best_model_v4.h5\n",
      "Epoch 15/100\n",
      "93935/93935 [==============================] - 475s 5ms/step - loss: 0.9792 - val_loss: 1.1105\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.14446 to 1.11054, saving model to best_model_v4.h5\n",
      "Epoch 16/100\n",
      "93935/93935 [==============================] - 490s 5ms/step - loss: 0.9312 - val_loss: 1.0849\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.11054 to 1.08494, saving model to best_model_v4.h5\n",
      "Epoch 17/100\n",
      "93935/93935 [==============================] - 470s 5ms/step - loss: 0.8866 - val_loss: 1.0570\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.08494 to 1.05704, saving model to best_model_v4.h5\n",
      "Epoch 18/100\n",
      "93935/93935 [==============================] - 485s 5ms/step - loss: 0.8449 - val_loss: 1.0318\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.05704 to 1.03178, saving model to best_model_v4.h5\n",
      "Epoch 19/100\n",
      "93935/93935 [==============================] - 482s 5ms/step - loss: 0.8062 - val_loss: 1.0158\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.03178 to 1.01584, saving model to best_model_v4.h5\n",
      "Epoch 20/100\n",
      "93935/93935 [==============================] - 496s 5ms/step - loss: 0.7707 - val_loss: 0.9905\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.01584 to 0.99051, saving model to best_model_v4.h5\n",
      "Epoch 21/100\n",
      "93935/93935 [==============================] - 498s 5ms/step - loss: 0.7376 - val_loss: 0.9743\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.99051 to 0.97433, saving model to best_model_v4.h5\n",
      "Epoch 22/100\n",
      "93935/93935 [==============================] - 442s 5ms/step - loss: 0.7071 - val_loss: 0.9625\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.97433 to 0.96254, saving model to best_model_v4.h5\n",
      "Epoch 23/100\n",
      "93935/93935 [==============================] - 303s 3ms/step - loss: 0.6780 - val_loss: 0.9515\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.96254 to 0.95154, saving model to best_model_v4.h5\n",
      "Epoch 24/100\n",
      "93935/93935 [==============================] - 298s 3ms/step - loss: 0.6515 - val_loss: 0.9417\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.95154 to 0.94173, saving model to best_model_v4.h5\n",
      "Epoch 25/100\n",
      "93935/93935 [==============================] - 300s 3ms/step - loss: 0.6260 - val_loss: 0.9341\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.94173 to 0.93411, saving model to best_model_v4.h5\n",
      "Epoch 26/100\n",
      "93935/93935 [==============================] - 301s 3ms/step - loss: 0.6022 - val_loss: 0.9257\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.93411 to 0.92571, saving model to best_model_v4.h5\n",
      "Epoch 27/100\n",
      "93935/93935 [==============================] - 301s 3ms/step - loss: 0.5797 - val_loss: 0.9187\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.92571 to 0.91865, saving model to best_model_v4.h5\n",
      "Epoch 28/100\n",
      "93935/93935 [==============================] - 302s 3ms/step - loss: 0.5583 - val_loss: 0.9158\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.91865 to 0.91584, saving model to best_model_v4.h5\n",
      "Epoch 29/100\n",
      "93935/93935 [==============================] - 303s 3ms/step - loss: 0.5382 - val_loss: 0.9147\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.91584 to 0.91467, saving model to best_model_v4.h5\n",
      "Epoch 30/100\n",
      "93935/93935 [==============================] - 304s 3ms/step - loss: 0.5184 - val_loss: 0.9086\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.91467 to 0.90857, saving model to best_model_v4.h5\n",
      "Epoch 31/100\n",
      "93935/93935 [==============================] - 303s 3ms/step - loss: 0.5005 - val_loss: 0.9057\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.90857 to 0.90573, saving model to best_model_v4.h5\n",
      "Epoch 32/100\n",
      "93935/93935 [==============================] - 304s 3ms/step - loss: 0.4832 - val_loss: 0.9021\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.90573 to 0.90209, saving model to best_model_v4.h5\n",
      "Epoch 33/100\n",
      "93935/93935 [==============================] - 303s 3ms/step - loss: 0.4668 - val_loss: 0.9048\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.90209\n",
      "Epoch 34/100\n",
      "93935/93935 [==============================] - 305s 3ms/step - loss: 0.4512 - val_loss: 0.9057\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.90209\n",
      "Epoch 35/100\n",
      "93935/93935 [==============================] - 304s 3ms/step - loss: 0.4366 - val_loss: 0.9061\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.90209\n",
      "Epoch 00035: early stopping\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:],\n",
    "                  epochs=100,callbacks=[es,mc], batch_size=512, \n",
    "                  validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model and save its weights\n",
    "model = load_model('pickle_and_trained_models/best_model_v4.h5')\n",
    "model.save_weights('pickle_and_trained_models/best_model_weights_v4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('best_model_weights_v4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap dictionary pairs\n",
    "reverse_target_word_index=dict((v, k) for k, v in target_word_index.items())\n",
    "reverse_source_word_index=dict((v, k) for k, v in source_word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "\n",
    "with open('pickle_and_trained_models/source_word_freq.pickle', 'wb') as handle:\n",
    "    pickle.dump(source_word_freq, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('pickle_and_trained_models/source_word_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(source_word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "   \n",
    "with open('pickle_and_trained_models/target_word_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(target_word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the context vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "## decoder_hidden_state_input = Input(shape=(max_spa_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "decoder_inputs_2 = Input(shape=(None,))\n",
    "dec_emb2= dec_emb_layer(decoder_inputs_2) \n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_2] + [decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.save(\"pickle_and_trained_models/docoder_model.h5\")\n",
    "encoder_model.save(\"pickle_and_trained_models/encoder_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['<start>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :]) \n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='<end>'):\n",
    "            decoded_sentence= decoded_sentence+sampled_token+' '\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == '<end>'  or len(decoded_sentence.split()) >= (max_eng_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate a few sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a sample from the validation data\n",
    "target = y_val_original[15000:15050].tolist()\n",
    "source = x_val_original[15000:15050].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=[]\n",
    "for i in x_val[15000:15050]:\n",
    "    predicted.append(decode_sequence(i.reshape(1,max_spa_len)))\n",
    "\n",
    "# add the actual and the predicted sentences to a dataframe\n",
    "df=pd.DataFrame({\"Source\":source,\"Target\":target,\"predicted\":predicted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>estoy muy agradecido por tu ayuda</td>\n",
       "      <td>i am very grateful for your help</td>\n",
       "      <td>i am very grateful for your help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tom no puede soportarlo</td>\n",
       "      <td>tom cant bear it</td>\n",
       "      <td>tom cant have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>el pez dorado esta vivo</td>\n",
       "      <td>the goldfish is alive</td>\n",
       "      <td>the bicycle is isnt beautiful tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ella se esta cepillando el pelo</td>\n",
       "      <td>she is brushing her hair</td>\n",
       "      <td>she is brushing her hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>debo advertirle</td>\n",
       "      <td>i must warn him</td>\n",
       "      <td>i must warn you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eres racista</td>\n",
       "      <td>are you racist</td>\n",
       "      <td>youre a doll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>un perro salto a la silla y se quedo ahi inmovil por cinco minutos</td>\n",
       "      <td>a dog jumped onto the chair and lay motionless for five minutes</td>\n",
       "      <td>a dog jumped over the window and left a bath and it left to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tom aguanto el dolor por un par de semanas antes de ir finalmente al hospital</td>\n",
       "      <td>tom put up with the pain for a couple of weeks before finally going to the hospital</td>\n",
       "      <td>i felt a little present when i visited her again for the hospital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>me respondio mi pregunta con un no</td>\n",
       "      <td>he answered my question with a no</td>\n",
       "      <td>i asked for my answer to question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>puedo usar tu coche seguro adelante</td>\n",
       "      <td>can i use your car sure go ahead</td>\n",
       "      <td>may i use your car for you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>estoy disfrutando mucho el reto</td>\n",
       "      <td>im really enjoying the challenge</td>\n",
       "      <td>im taking a &lt;unk&gt; japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>estoy comenzando a extranar a mi novia</td>\n",
       "      <td>im beginning to miss my girlfriend</td>\n",
       "      <td>im beginning to miss my job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>llevare a arreglar estos zapatos antes de manana</td>\n",
       "      <td>i will get these shoes repaired by tomorrow</td>\n",
       "      <td>ill take these shoes before going to get to my office today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tom esta siendo perseguido por la policia</td>\n",
       "      <td>tom is being hunted by the police</td>\n",
       "      <td>tom is being a &lt;unk&gt; of the police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>yo recomendaria no hacerlo</td>\n",
       "      <td>id recommend not doing that</td>\n",
       "      <td>i wouldnt do that you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ella planea terminar con su novio</td>\n",
       "      <td>she plans to break up with her boyfriend</td>\n",
       "      <td>she has plans to give her on an offer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tom pasaba horas mirando a los peces en el estanque</td>\n",
       "      <td>tom spent hours looking at the fish in the tank</td>\n",
       "      <td>tom used to read in the mountains in the summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>aunque es listo no es sabio</td>\n",
       "      <td>though he is clever he isnt wise</td>\n",
       "      <td>even though its not our fault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>no me dejas eleccion</td>\n",
       "      <td>you leave me no choice</td>\n",
       "      <td>dont you let me know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mary se limo las unas</td>\n",
       "      <td>mary filed her nails</td>\n",
       "      <td>mary stood up with a &lt;unk&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tuve un sueno extrano anoche</td>\n",
       "      <td>last night i had a weird dream</td>\n",
       "      <td>i had a dream of having been home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tenga cuidado no deje caer la bandeja</td>\n",
       "      <td>be careful dont drop the tray</td>\n",
       "      <td>be careful not &lt;unk&gt; the alarm you will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>el nacio en ohio</td>\n",
       "      <td>he was born in ohio</td>\n",
       "      <td>he was born in &lt;unk&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tom encerro a su hermana en el armario</td>\n",
       "      <td>tom locked his sister in the closet</td>\n",
       "      <td>tom didnt fall in marys bedroom life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>me gustas mucho</td>\n",
       "      <td>i like you a lot</td>\n",
       "      <td>i like this very much tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>el me dijo su direccion pero desafortunadamente yo no tenia un papel en que anotarlo</td>\n",
       "      <td>he told me his address but unfortunately i had no paper to write it down on</td>\n",
       "      <td>he told me that by my address credit curiosity fallen wouldnt with them have a heavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hay alguien afuera</td>\n",
       "      <td>someones outside</td>\n",
       "      <td>is there anyone out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aprendimos nuestra leccion</td>\n",
       "      <td>weve learned our lesson</td>\n",
       "      <td>we learned this lesson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>estaria encantado de ir en tu lugar</td>\n",
       "      <td>id be glad to go in your place</td>\n",
       "      <td>id be glad to go with your place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ellos nos conocen</td>\n",
       "      <td>they know us</td>\n",
       "      <td>they know us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>pruebate esa camisa</td>\n",
       "      <td>try on that shirt</td>\n",
       "      <td>try on this shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>aquella senora que lleva un bebe es mi mujer</td>\n",
       "      <td>the woman over there holding a baby is my wife</td>\n",
       "      <td>that woman has become a woman in my baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>el intento atraer su atencion</td>\n",
       "      <td>he tried to attract her attention</td>\n",
       "      <td>he tried to his attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tom es irresistible</td>\n",
       "      <td>tom is irresistible</td>\n",
       "      <td>tom is all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>la unica regla de oro es que el que tiene el oro hace las reglas</td>\n",
       "      <td>the only golden rule is that he who has the gold makes the rules</td>\n",
       "      <td>the plan that gold is the gold but that was the gold of that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tom no nos va a decir nada</td>\n",
       "      <td>tom wont tell us anything</td>\n",
       "      <td>tom wont tell us anything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>metete en el auto y esperame</td>\n",
       "      <td>get into the car and wait for me</td>\n",
       "      <td>get in the car and get dressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>mi veterinario no le da de comer a su perro comida de perros comercial</td>\n",
       "      <td>my vet wont feed his dog commercial dog food</td>\n",
       "      <td>my dog is trying to feed his dog in that morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>maria es muy chula</td>\n",
       "      <td>mary is a fox</td>\n",
       "      <td>mary is very &lt;unk&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ellos gritaron tan fuerte como pudieron</td>\n",
       "      <td>they shouted as loudly as they could</td>\n",
       "      <td>they tried it as as they were almost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>usted es la unica que lo compro</td>\n",
       "      <td>youre the only one who bought it</td>\n",
       "      <td>youre the only man you buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>le llevo a tom varias horas ensamblar una cama litera que las instruccciones decian que lleva menos de una hora de montaje</td>\n",
       "      <td>it took tom several hours to assemble a bunk bed that the instructions said would take less than an hour to assemble</td>\n",
       "      <td>tom took his &lt;unk&gt; to take his back of books to get to every sight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>pienso que tom podria saber algo</td>\n",
       "      <td>i think tom might know something</td>\n",
       "      <td>i think tom might know something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>esto esta perfecto</td>\n",
       "      <td>this is fine</td>\n",
       "      <td>this is perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>es un problema importante</td>\n",
       "      <td>its a serious problem</td>\n",
       "      <td>its a problem important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>pienso que es poco saludable comer mas de naranjas en un dia</td>\n",
       "      <td>i think its unhealthy to eat more than oranges a day</td>\n",
       "      <td>i think that eating is a little more expensive eat than six dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>si empezamos temprano podemos terminar para el almuerzo</td>\n",
       "      <td>if we begin early we can finish by lunch</td>\n",
       "      <td>we will get up early as long as we used to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>se tu edad</td>\n",
       "      <td>i know how old you are</td>\n",
       "      <td>im your age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>te vi en la tele</td>\n",
       "      <td>i saw you on tv</td>\n",
       "      <td>i saw you on tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>la escuela deberia eliminar el uniforme</td>\n",
       "      <td>the school should do away with uniforms</td>\n",
       "      <td>school are my options to wear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        Source  \\\n",
       "0                                                                                            estoy muy agradecido por tu ayuda   \n",
       "1                                                                                                      tom no puede soportarlo   \n",
       "2                                                                                                      el pez dorado esta vivo   \n",
       "3                                                                                              ella se esta cepillando el pelo   \n",
       "4                                                                                                              debo advertirle   \n",
       "5                                                                                                                 eres racista   \n",
       "6                                                           un perro salto a la silla y se quedo ahi inmovil por cinco minutos   \n",
       "7                                                tom aguanto el dolor por un par de semanas antes de ir finalmente al hospital   \n",
       "8                                                                                           me respondio mi pregunta con un no   \n",
       "9                                                                                          puedo usar tu coche seguro adelante   \n",
       "10                                                                                             estoy disfrutando mucho el reto   \n",
       "11                                                                                      estoy comenzando a extranar a mi novia   \n",
       "12                                                                            llevare a arreglar estos zapatos antes de manana   \n",
       "13                                                                                   tom esta siendo perseguido por la policia   \n",
       "14                                                                                                  yo recomendaria no hacerlo   \n",
       "15                                                                                           ella planea terminar con su novio   \n",
       "16                                                                         tom pasaba horas mirando a los peces en el estanque   \n",
       "17                                                                                                 aunque es listo no es sabio   \n",
       "18                                                                                                        no me dejas eleccion   \n",
       "19                                                                                                       mary se limo las unas   \n",
       "20                                                                                                tuve un sueno extrano anoche   \n",
       "21                                                                                       tenga cuidado no deje caer la bandeja   \n",
       "22                                                                                                            el nacio en ohio   \n",
       "23                                                                                      tom encerro a su hermana en el armario   \n",
       "24                                                                                                             me gustas mucho   \n",
       "25                                        el me dijo su direccion pero desafortunadamente yo no tenia un papel en que anotarlo   \n",
       "26                                                                                                          hay alguien afuera   \n",
       "27                                                                                                  aprendimos nuestra leccion   \n",
       "28                                                                                         estaria encantado de ir en tu lugar   \n",
       "29                                                                                                           ellos nos conocen   \n",
       "30                                                                                                         pruebate esa camisa   \n",
       "31                                                                                aquella senora que lleva un bebe es mi mujer   \n",
       "32                                                                                               el intento atraer su atencion   \n",
       "33                                                                                                         tom es irresistible   \n",
       "34                                                            la unica regla de oro es que el que tiene el oro hace las reglas   \n",
       "35                                                                                                  tom no nos va a decir nada   \n",
       "36                                                                                                metete en el auto y esperame   \n",
       "37                                                      mi veterinario no le da de comer a su perro comida de perros comercial   \n",
       "38                                                                                                          maria es muy chula   \n",
       "39                                                                                     ellos gritaron tan fuerte como pudieron   \n",
       "40                                                                                             usted es la unica que lo compro   \n",
       "41  le llevo a tom varias horas ensamblar una cama litera que las instruccciones decian que lleva menos de una hora de montaje   \n",
       "42                                                                                            pienso que tom podria saber algo   \n",
       "43                                                                                                          esto esta perfecto   \n",
       "44                                                                                                   es un problema importante   \n",
       "45                                                                pienso que es poco saludable comer mas de naranjas en un dia   \n",
       "46                                                                     si empezamos temprano podemos terminar para el almuerzo   \n",
       "47                                                                                                                  se tu edad   \n",
       "48                                                                                                            te vi en la tele   \n",
       "49                                                                                     la escuela deberia eliminar el uniforme   \n",
       "\n",
       "                                                                                                                  Target  \\\n",
       "0                                                                                       i am very grateful for your help   \n",
       "1                                                                                                       tom cant bear it   \n",
       "2                                                                                                  the goldfish is alive   \n",
       "3                                                                                               she is brushing her hair   \n",
       "4                                                                                                        i must warn him   \n",
       "5                                                                                                         are you racist   \n",
       "6                                                        a dog jumped onto the chair and lay motionless for five minutes   \n",
       "7                                    tom put up with the pain for a couple of weeks before finally going to the hospital   \n",
       "8                                                                                      he answered my question with a no   \n",
       "9                                                                                       can i use your car sure go ahead   \n",
       "10                                                                                      im really enjoying the challenge   \n",
       "11                                                                                    im beginning to miss my girlfriend   \n",
       "12                                                                           i will get these shoes repaired by tomorrow   \n",
       "13                                                                                     tom is being hunted by the police   \n",
       "14                                                                                           id recommend not doing that   \n",
       "15                                                                              she plans to break up with her boyfriend   \n",
       "16                                                                       tom spent hours looking at the fish in the tank   \n",
       "17                                                                                      though he is clever he isnt wise   \n",
       "18                                                                                                you leave me no choice   \n",
       "19                                                                                                  mary filed her nails   \n",
       "20                                                                                        last night i had a weird dream   \n",
       "21                                                                                         be careful dont drop the tray   \n",
       "22                                                                                                   he was born in ohio   \n",
       "23                                                                                   tom locked his sister in the closet   \n",
       "24                                                                                                      i like you a lot   \n",
       "25                                           he told me his address but unfortunately i had no paper to write it down on   \n",
       "26                                                                                                      someones outside   \n",
       "27                                                                                               weve learned our lesson   \n",
       "28                                                                                        id be glad to go in your place   \n",
       "29                                                                                                          they know us   \n",
       "30                                                                                                     try on that shirt   \n",
       "31                                                                        the woman over there holding a baby is my wife   \n",
       "32                                                                                     he tried to attract her attention   \n",
       "33                                                                                                   tom is irresistible   \n",
       "34                                                      the only golden rule is that he who has the gold makes the rules   \n",
       "35                                                                                             tom wont tell us anything   \n",
       "36                                                                                      get into the car and wait for me   \n",
       "37                                                                          my vet wont feed his dog commercial dog food   \n",
       "38                                                                                                         mary is a fox   \n",
       "39                                                                                  they shouted as loudly as they could   \n",
       "40                                                                                      youre the only one who bought it   \n",
       "41  it took tom several hours to assemble a bunk bed that the instructions said would take less than an hour to assemble   \n",
       "42                                                                                      i think tom might know something   \n",
       "43                                                                                                          this is fine   \n",
       "44                                                                                                 its a serious problem   \n",
       "45                                                                  i think its unhealthy to eat more than oranges a day   \n",
       "46                                                                              if we begin early we can finish by lunch   \n",
       "47                                                                                                i know how old you are   \n",
       "48                                                                                                       i saw you on tv   \n",
       "49                                                                               the school should do away with uniforms   \n",
       "\n",
       "                                                                               predicted  \n",
       "0                                                       i am very grateful for your help  \n",
       "1                                                                       tom cant have it  \n",
       "2                                                  the bicycle is isnt beautiful tonight  \n",
       "3                                                               she is brushing her hair  \n",
       "4                                                                        i must warn you  \n",
       "5                                                                           youre a doll  \n",
       "6                            a dog jumped over the window and left a bath and it left to  \n",
       "7                      i felt a little present when i visited her again for the hospital  \n",
       "8                                                      i asked for my answer to question  \n",
       "9                                                             may i use your car for you  \n",
       "10                                                            im taking a <unk> japanese  \n",
       "11                                                           im beginning to miss my job  \n",
       "12                           ill take these shoes before going to get to my office today  \n",
       "13                                                    tom is being a <unk> of the police  \n",
       "14                                                                 i wouldnt do that you  \n",
       "15                                                 she has plans to give her on an offer  \n",
       "16                                       tom used to read in the mountains in the summer  \n",
       "17                                                         even though its not our fault  \n",
       "18                                                                  dont you let me know  \n",
       "19                                                            mary stood up with a <unk>  \n",
       "20                                                     i had a dream of having been home  \n",
       "21                                               be careful not <unk> the alarm you will  \n",
       "22                                                                  he was born in <unk>  \n",
       "23                                                  tom didnt fall in marys bedroom life  \n",
       "24                                                             i like this very much tea  \n",
       "25  he told me that by my address credit curiosity fallen wouldnt with them have a heavy  \n",
       "26                                                                   is there anyone out  \n",
       "27                                                                we learned this lesson  \n",
       "28                                                      id be glad to go with your place  \n",
       "29                                                                          they know us  \n",
       "30                                                                     try on this shirt  \n",
       "31                                              that woman has become a woman in my baby  \n",
       "32                                                             he tried to his attention  \n",
       "33                                                                            tom is all  \n",
       "34                          the plan that gold is the gold but that was the gold of that  \n",
       "35                                                             tom wont tell us anything  \n",
       "36                                                        get in the car and get dressed  \n",
       "37                                      my dog is trying to feed his dog in that morning  \n",
       "38                                                                    mary is very <unk>  \n",
       "39                                                  they tried it as as they were almost  \n",
       "40                                                            youre the only man you buy  \n",
       "41                    tom took his <unk> to take his back of books to get to every sight  \n",
       "42                                                      i think tom might know something  \n",
       "43                                                                       this is perfect  \n",
       "44                                                               its a problem important  \n",
       "45                      i think that eating is a little more expensive eat than six dogs  \n",
       "46                                            we will get up early as long as we used to  \n",
       "47                                                                           im your age  \n",
       "48                                                                       i saw you on tv  \n",
       "49                                                         school are my options to wear  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
